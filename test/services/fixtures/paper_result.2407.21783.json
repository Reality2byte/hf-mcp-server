[
  {
    "paper": {
      "id": "2407.21783",
      "authors": [
        {
          "_id": "66aaf835636a86cff6f5e7b3",
          "user": {
            "_id": "6376f5a98272ecd8ddacacf8",
            "avatarUrl": "/avatars/c57d5de9a90ec12b7f617327b1ac27fd.svg",
            "isPro": false,
            "fullname": "Abhimanyu Dubey",
            "user": "dubeya",
            "type": "user"
          },
          "name": "Abhimanyu Dubey",
          "status": "admin_assigned",
          "statusLastChangedAt": "2024-08-01T15:39:47.405Z",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7b4",
          "name": "Abhinav Jauhri",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7b5",
          "name": "Abhinav Pandey",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7b6",
          "user": {
            "_id": "62c88a674fd4a24e1523f478",
            "avatarUrl": "/avatars/5a21438eca713bd97f5a2c8c5cbd980a.svg",
            "isPro": false,
            "fullname": "Abhishek Kadian",
            "user": "abhiskk",
            "type": "user"
          },
          "name": "Abhishek Kadian",
          "status": "admin_assigned",
          "statusLastChangedAt": "2024-08-01T15:44:58.140Z",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7b7",
          "name": "Ahmad Al-Dahle",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7b8",
          "name": "Aiesha Letman",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7b9",
          "name": "Akhil Mathur",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7ba",
          "user": {
            "_id": "64ddf47b5a8a9efea8d6b1b7",
            "avatarUrl": "/avatars/0a2f1b581235942d40c9b3f37464aa64.svg",
            "isPro": false,
            "fullname": "Alan Schelten",
            "user": "altrevr",
            "type": "user"
          },
          "name": "Alan Schelten",
          "status": "admin_assigned",
          "statusLastChangedAt": "2024-08-01T15:46:01.561Z",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7bb",
          "name": "Amy Yang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7bc",
          "user": {
            "_id": "6089199a2b1d353b2f4c9919",
            "avatarUrl": "/avatars/de1bb22e89090116f5f97b5a2e061cea.svg",
            "isPro": false,
            "fullname": "Angela Fan",
            "user": "angelafan",
            "type": "user"
          },
          "name": "Angela Fan",
          "status": "admin_assigned",
          "statusLastChangedAt": "2024-08-01T15:51:05.219Z",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7bd",
          "name": "Anirudh Goyal",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7be",
          "name": "Anthony Hartshorn",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7bf",
          "name": "Aobo Yang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7c0",
          "name": "Archi Mitra",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7c1",
          "name": "Archie Sravankumar",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7c2",
          "name": "Artem Korenev",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7c3",
          "name": "Arthur Hinsvark",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7c4",
          "name": "Arun Rao",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7c5",
          "name": "Aston Zhang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7c6",
          "name": "Aurelien Rodriguez",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7c7",
          "name": "Austen Gregerson",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7c8",
          "name": "Ava Spataru",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7c9",
          "name": "Baptiste Roziere",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7ca",
          "name": "Bethany Biron",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7cb",
          "name": "Binh Tang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7cc",
          "name": "Bobbie Chern",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7cd",
          "name": "Charlotte Caucheteux",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7ce",
          "name": "Chaya Nayak",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7cf",
          "name": "Chloe Bi",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7d0",
          "name": "Chris Marra",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7d1",
          "name": "Chris McConnell",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7d2",
          "name": "Christian Keller",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7d3",
          "name": "Christophe Touret",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7d4",
          "name": "Chunyang Wu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7d5",
          "name": "Corinne Wong",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7d6",
          "name": "Cristian Canton Ferrer",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7d7",
          "user": {
            "_id": "63153d9a27d020853f67f061",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63153d9a27d020853f67f061/qKFIxr6mmga1yktDovRwI.jpeg",
            "isPro": false,
            "fullname": "Cyrus Nikolaidis",
            "user": "cynikolai",
            "type": "user"
          },
          "name": "Cyrus Nikolaidis",
          "status": "claimed_verified",
          "statusLastChangedAt": "2024-10-02T07:41:42.533Z",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7d8",
          "name": "Damien Allonsius",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7d9",
          "name": "Daniel Song",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7da",
          "name": "Danielle Pintz",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7db",
          "name": "Danny Livshits",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7dc",
          "name": "David Esiobu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7dd",
          "name": "Dhruv Choudhary",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7de",
          "name": "Dhruv Mahajan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7df",
          "name": "Diego Garcia-Olano",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7e0",
          "name": "Diego Perino",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7e1",
          "name": "Dieuwke Hupkes",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7e2",
          "name": "Egor Lakomkin",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7e3",
          "name": "Ehab AlBadawy",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7e4",
          "name": "Elina Lobanova",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7e5",
          "name": "Emily Dinan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7e6",
          "name": "Eric Michael Smith",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7e7",
          "name": "Filip Radenovic",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7e8",
          "name": "Frank Zhang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7e9",
          "name": "Gabriel Synnaeve",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7ea",
          "name": "Gabrielle Lee",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7eb",
          "name": "Georgia Lewis Anderson",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7ec",
          "name": "Graeme Nail",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7ed",
          "name": "Gregoire Mialon",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7ee",
          "name": "Guan Pang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7ef",
          "name": "Guillem Cucurell",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7f0",
          "name": "Hailey Nguyen",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7f1",
          "name": "Hannah Korevaar",
          "hidden": false
        },
        { "_id": "66aaf835636a86cff6f5e7f2", "name": "Hu Xu", "hidden": false },
        {
          "_id": "66aaf835636a86cff6f5e7f3",
          "name": "Hugo Touvron",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7f4",
          "name": "Iliyan Zarov",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7f5",
          "name": "Imanol Arrieta Ibarra",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7f6",
          "name": "Isabel Kloumann",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7f7",
          "name": "Ishan Misra",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7f8",
          "name": "Ivan Evtimov",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7f9",
          "name": "Jade Copet",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7fa",
          "name": "Jaewon Lee",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7fb",
          "name": "Jan Geffert",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7fc",
          "name": "Jana Vranes",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7fd",
          "name": "Jason Park",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7fe",
          "name": "Jay Mahadeokar",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e7ff",
          "name": "Jeet Shah",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e800",
          "name": "Jelmer van der Linde",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e801",
          "name": "Jennifer Billock",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e802",
          "name": "Jenny Hong",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e803",
          "name": "Jenya Lee",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e804",
          "name": "Jeremy Fu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e805",
          "name": "Jianfeng Chi",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e806",
          "name": "Jianyu Huang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e807",
          "name": "Jiawen Liu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e808",
          "name": "Jie Wang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e809",
          "name": "Jiecao Yu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e80a",
          "name": "Joanna Bitton",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e80b",
          "name": "Joe Spisak",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e80c",
          "name": "Jongsoo Park",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e80d",
          "name": "Joseph Rocca",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e80e",
          "name": "Joshua Johnstun",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e80f",
          "name": "Joshua Saxe",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e810",
          "name": "Junteng Jia",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e811",
          "name": "Kalyan Vasuden Alwala",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e812",
          "name": "Kartikeya Upasani",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e813",
          "name": "Kate Plawiak",
          "hidden": false
        },
        { "_id": "66aaf835636a86cff6f5e814", "name": "Ke Li", "hidden": false },
        {
          "_id": "66aaf835636a86cff6f5e815",
          "name": "Kenneth Heafield",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e816",
          "name": "Kevin Stone",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e817",
          "name": "Khalid El-Arini",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e818",
          "name": "Krithika Iyer",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e819",
          "name": "Kshitiz Malik",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e81a",
          "name": "Kuenley Chiu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e81b",
          "name": "Kunal Bhalla",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e81c",
          "name": "Lauren Rantala-Yeary",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e81d",
          "user": {
            "_id": "6311629dbc152fa3e8144e1e",
            "avatarUrl": "/avatars/27f325beae3ee09f70fe4c341ca24790.svg",
            "isPro": false,
            "fullname": "Laurens",
            "user": "lvdmaaten",
            "type": "user"
          },
          "name": "Laurens van der Maaten",
          "status": "claimed_verified",
          "statusLastChangedAt": "2024-10-03T08:31:56.548Z",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e81e",
          "name": "Lawrence Chen",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e81f",
          "name": "Liang Tan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e820",
          "name": "Liz Jenkins",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e821",
          "name": "Louis Martin",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e822",
          "name": "Lovish Madaan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e823",
          "name": "Lubo Malo",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e824",
          "name": "Lukas Blecher",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e825",
          "name": "Lukas Landzaat",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e826",
          "name": "Luke de Oliveira",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e827",
          "name": "Madeline Muzzi",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e828",
          "name": "Mahesh Pasupuleti",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e829",
          "user": {
            "_id": "650fece35877b1c077298b81",
            "avatarUrl": "/avatars/be9c948ecf9c413ad4c8aa6598d3ba86.svg",
            "isPro": false,
            "fullname": "Mannat Singh",
            "user": "mannatsingh",
            "type": "user"
          },
          "name": "Mannat Singh",
          "status": "claimed_verified",
          "statusLastChangedAt": "2024-08-03T08:25:58.343Z",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e82a",
          "name": "Manohar Paluri",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e82b",
          "name": "Marcin Kardas",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e82c",
          "name": "Mathew Oldham",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e82d",
          "name": "Mathieu Rita",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e82e",
          "name": "Maya Pavlova",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e82f",
          "name": "Melanie Kambadur",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e830",
          "name": "Mike Lewis",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e831",
          "name": "Min Si",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e832",
          "name": "Mitesh Kumar Singh",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e833",
          "name": "Mona Hassan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e834",
          "name": "Naman Goyal",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e835",
          "name": "Narjes Torabi",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e836",
          "user": {
            "_id": "633476edc3cb9eda9328e556",
            "avatarUrl": "/avatars/a127e270a606c18623fe00cd723313f6.svg",
            "isPro": false,
            "fullname": "Nikolay B",
            "user": "bashnick",
            "type": "user"
          },
          "name": "Nikolay Bashlykov",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T14:43:28.192Z",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e837",
          "name": "Nikolay Bogoychev",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e838",
          "name": "Niladri Chatterji",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e839",
          "name": "Olivier Duchenne",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e83a",
          "name": "Onur Çelebi",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e83b",
          "name": "Patrick Alrassy",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e83c",
          "name": "Pengchuan Zhang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e83d",
          "name": "Pengwei Li",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e83e",
          "name": "Petar Vasic",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e83f",
          "name": "Peter Weng",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e840",
          "name": "Prajjwal Bhargava",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e841",
          "name": "Pratik Dubal",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e842",
          "name": "Praveen Krishnan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e843",
          "name": "Punit Singh Koura",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e844",
          "name": "Puxin Xu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e845",
          "name": "Qing He",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e846",
          "name": "Qingxiao Dong",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e847",
          "name": "Ragavan Srinivasan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e848",
          "name": "Raj Ganapathy",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e849",
          "name": "Ramon Calderer",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e84a",
          "user": {
            "_id": "67b8749ffa8442592bce008e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ctbzupAxRhcNXDka75ANi.png",
            "isPro": false,
            "fullname": "Ricardo Silveira Cabral",
            "user": "rscabral",
            "type": "user"
          },
          "name": "Ricardo Silveira Cabral",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-21T14:43:25.770Z",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e84b",
          "name": "Robert Stojnic",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e84c",
          "name": "Roberta Raileanu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e84d",
          "name": "Rohit Girdhar",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e84e",
          "name": "Rohit Patel",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e84f",
          "name": "Romain Sauvestre",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e850",
          "name": "Ronnie Polidoro",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e851",
          "name": "Roshan Sumbaly",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e852",
          "user": {
            "_id": "64b808d9ff6d81ae29870b52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b808d9ff6d81ae29870b52/Oo040VGSVy-9uuzFAb-GU.jpeg",
            "isPro": false,
            "fullname": "Ross T",
            "user": "RJT1990",
            "type": "user"
          },
          "name": "Ross Taylor",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-12T09:19:26.421Z",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e853",
          "name": "Ruan Silva",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e854",
          "name": "Rui Hou",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e855",
          "name": "Rui Wang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e856",
          "name": "Saghar Hosseini",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e857",
          "name": "Sahana Chennabasappa",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e858",
          "name": "Sanjay Singh",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e859",
          "name": "Sean Bell",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e85a",
          "name": "Seohyun Sonia Kim",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e85b",
          "name": "Sergey Edunov",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e85c",
          "name": "Shaoliang Nie",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e85d",
          "name": "Sharan Narang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e85e",
          "name": "Sharath Raparthy",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e85f",
          "name": "Sheng Shen",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e860",
          "name": "Shengye Wan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e861",
          "name": "Shruti Bhosale",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e862",
          "name": "Shun Zhang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e863",
          "name": "Simon Vandenhende",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e864",
          "name": "Soumya Batra",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e865",
          "name": "Spencer Whitman",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e866",
          "name": "Sten Sootla",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e867",
          "name": "Stephane Collot",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e868",
          "name": "Suchin Gururangan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e869",
          "name": "Sydney Borodinsky",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e86a",
          "name": "Tamar Herman",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e86b",
          "name": "Tara Fowler",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e86c",
          "name": "Tarek Sheasha",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e86d",
          "name": "Thomas Georgiou",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e86e",
          "name": "Thomas Scialom",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e86f",
          "name": "Tobias Speckbacher",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e870",
          "name": "Todor Mihaylov",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e871",
          "name": "Tong Xiao",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e872",
          "name": "Ujjwal Karn",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e873",
          "name": "Vedanuj Goswami",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e874",
          "name": "Vibhor Gupta",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e875",
          "name": "Vignesh Ramanathan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e876",
          "name": "Viktor Kerkez",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e877",
          "name": "Vincent Gonguet",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e878",
          "name": "Virginie Do",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e879",
          "name": "Vish Vogeti",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e87a",
          "name": "Vladan Petrovic",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e87b",
          "name": "Weiwei Chu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e87c",
          "name": "Wenhan Xiong",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e87d",
          "name": "Wenyin Fu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e87e",
          "name": "Whitney Meers",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e87f",
          "name": "Xavier Martinet",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e880",
          "name": "Xiaodong Wang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e881",
          "name": "Xiaoqing Ellen Tan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e882",
          "name": "Xinfeng Xie",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e883",
          "name": "Xuchao Jia",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e884",
          "name": "Xuewei Wang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e885",
          "name": "Yaelle Goldschlag",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e886",
          "name": "Yashesh Gaur",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e887",
          "name": "Yasmine Babaei",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e888",
          "name": "Yi Wen",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e889",
          "name": "Yiwen Song",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e88a",
          "name": "Yuchen Zhang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e88b",
          "name": "Yue Li",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e88c",
          "name": "Yuning Mao",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e88d",
          "name": "Zacharie Delpierre Coudert",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e88e",
          "name": "Zheng Yan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e88f",
          "name": "Zhengxing Chen",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e890",
          "name": "Zoe Papakipos",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e891",
          "name": "Aaditya Singh",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e892",
          "name": "Aaron Grattafiori",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e893",
          "name": "Abha Jain",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e894",
          "name": "Adam Kelsey",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e895",
          "name": "Adam Shajnfeld",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e896",
          "name": "Adithya Gangidi",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e897",
          "name": "Adolfo Victoria",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e898",
          "name": "Ahuva Goldstand",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e899",
          "name": "Ajay Menon",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e89a",
          "name": "Ajay Sharma",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e89b",
          "name": "Alex Boesenberg",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e89c",
          "name": "Alex Vaughan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e89d",
          "name": "Alexei Baevski",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e89e",
          "name": "Allie Feinstein",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e89f",
          "name": "Amanda Kallet",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8a0",
          "name": "Amit Sangani",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8a1",
          "name": "Anam Yunus",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8a2",
          "name": "Andrei Lupu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8a3",
          "name": "Andres Alvarado",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8a4",
          "name": "Andrew Caples",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8a5",
          "name": "Andrew Gu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8a6",
          "name": "Andrew Ho",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8a7",
          "name": "Andrew Poulton",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8a8",
          "name": "Andrew Ryan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8a9",
          "name": "Ankit Ramchandani",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8aa",
          "name": "Annie Franco",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8ab",
          "name": "Aparajita Saraf",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8ac",
          "name": "Arkabandhu Chowdhury",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8ad",
          "name": "Ashley Gabriel",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8ae",
          "name": "Ashwin Bharambe",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8af",
          "name": "Assaf Eisenman",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8b0",
          "name": "Azadeh Yazdan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8b1",
          "name": "Beau James",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8b2",
          "name": "Ben Maurer",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8b3",
          "name": "Benjamin Leonhardi",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8b4",
          "name": "Bernie Huang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8b5",
          "name": "Beth Loyd",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8b6",
          "name": "Beto De Paola",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8b7",
          "name": "Bhargavi Paranjape",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8b8",
          "name": "Bing Liu",
          "hidden": false
        },
        { "_id": "66aaf835636a86cff6f5e8b9", "name": "Bo Wu", "hidden": false },
        {
          "_id": "66aaf835636a86cff6f5e8ba",
          "name": "Boyu Ni",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8bb",
          "name": "Braden Hancock",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8bc",
          "name": "Bram Wasti",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8bd",
          "name": "Brandon Spence",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8be",
          "name": "Brani Stojkovic",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8bf",
          "name": "Brian Gamido",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8c0",
          "name": "Britt Montalvo",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8c1",
          "name": "Carl Parker",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8c2",
          "name": "Carly Burton",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8c3",
          "name": "Catalina Mejia",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8c4",
          "name": "Changhan Wang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8c5",
          "name": "Changkyu Kim",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8c6",
          "name": "Chao Zhou",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8c7",
          "name": "Chester Hu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8c8",
          "name": "Ching-Hsiang Chu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8c9",
          "name": "Chris Cai",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8ca",
          "name": "Chris Tindal",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8cb",
          "name": "Christoph Feichtenhofer",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8cc",
          "name": "Damon Civin",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8cd",
          "name": "Dana Beaty",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8ce",
          "name": "Daniel Kreymer",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8cf",
          "name": "Daniel Li",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8d0",
          "name": "Danny Wyatt",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8d1",
          "name": "David Adkins",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8d2",
          "name": "David Xu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8d3",
          "name": "Davide Testuggine",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8d4",
          "name": "Delia David",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8d5",
          "name": "Devi Parikh",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8d6",
          "name": "Diana Liskovich",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8d7",
          "name": "Didem Foss",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8d8",
          "name": "Dingkang Wang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8d9",
          "name": "Duc Le",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8da",
          "name": "Dustin Holland",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8db",
          "name": "Edward Dowling",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8dc",
          "name": "Eissa Jamil",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8dd",
          "name": "Elaine Montgomery",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8de",
          "name": "Eleonora Presani",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8df",
          "name": "Emily Hahn",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8e0",
          "name": "Emily Wood",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8e1",
          "name": "Erik Brinkman",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8e2",
          "name": "Esteban Arcaute",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8e3",
          "name": "Evan Dunbar",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8e4",
          "name": "Evan Smothers",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8e5",
          "name": "Fei Sun",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8e6",
          "name": "Felix Kreuk",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8e7",
          "name": "Feng Tian",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8e8",
          "name": "Firat Ozgenel",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8e9",
          "name": "Francesco Caggioni",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8ea",
          "name": "Francisco Guzmán",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8eb",
          "name": "Frank Kanayet",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8ec",
          "name": "Frank Seide",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8ed",
          "name": "Gabriela Medina Florez",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8ee",
          "name": "Gabriella Schwarz",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8ef",
          "name": "Gada Badeer",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8f0",
          "name": "Georgia Swee",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8f1",
          "name": "Gil Halpern",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8f2",
          "name": "Govind Thattai",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8f3",
          "name": "Grant Herman",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8f4",
          "name": "Grigory Sizov",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8f5",
          "name": "Guangyi",
          "hidden": false
        },
        { "_id": "66aaf835636a86cff6f5e8f6", "name": "Zhang", "hidden": false },
        {
          "_id": "66aaf835636a86cff6f5e8f7",
          "name": "Guna Lakshminarayanan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8f8",
          "name": "Hamid Shojanazeri",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8f9",
          "name": "Han Zou",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8fa",
          "name": "Hannah Wang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8fb",
          "name": "Hanwen Zha",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8fc",
          "name": "Haroun Habeeb",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8fd",
          "name": "Harrison Rudolph",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8fe",
          "name": "Helen Suk",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e8ff",
          "name": "Henry Aspegren",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e900",
          "name": "Hunter Goldman",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e901",
          "name": "Igor Molybog",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e902",
          "name": "Igor Tufanov",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e903",
          "name": "Irina-Elena Veliche",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e904",
          "name": "Itai Gat",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e905",
          "name": "Jake Weissman",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e906",
          "name": "James Geboski",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e907",
          "name": "James Kohli",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e908",
          "name": "Japhet Asher",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e909",
          "name": "Jean-Baptiste Gaya",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e90a",
          "name": "Jeff Marcus",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e90b",
          "name": "Jeff Tang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e90c",
          "name": "Jennifer Chan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e90d",
          "name": "Jenny Zhen",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e90e",
          "name": "Jeremy Reizenstein",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e90f",
          "name": "Jeremy Teboul",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e910",
          "name": "Jessica Zhong",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e911",
          "name": "Jian Jin",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e912",
          "name": "Jingyi Yang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e913",
          "name": "Joe Cummings",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e914",
          "name": "Jon Carvill",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e915",
          "name": "Jon Shepard",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e916",
          "name": "Jonathan McPhie",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e917",
          "name": "Jonathan Torres",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e918",
          "name": "Josh Ginsburg",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e919",
          "name": "Junjie Wang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e91a",
          "user": {
            "_id": "660b40d048408366ccbe0769",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660b40d048408366ccbe0769/MyWwlqNMHXc9gb_ygYKHs.jpeg",
            "isPro": false,
            "fullname": "Kai Wu",
            "user": "wukaixingxp",
            "type": "user"
          },
          "name": "Kai Wu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2024-08-09T17:43:13.267Z",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e91b",
          "name": "Kam Hou U",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e91c",
          "user": {
            "_id": "6601b3c3d2a378a163654efc",
            "avatarUrl": "/avatars/9ae79093b80997de76f1e57c134548d0.svg",
            "isPro": false,
            "fullname": "Karan Saxena",
            "user": "weight-matrix",
            "type": "user"
          },
          "name": "Karan Saxena",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-06T09:28:25.730Z",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e91d",
          "name": "Karthik Prasad",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e91e",
          "name": "Kartikay Khandelwal",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e91f",
          "name": "Katayoun Zand",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e920",
          "name": "Kathy Matosich",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e921",
          "name": "Kaushik Veeraraghavan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e922",
          "name": "Kelly Michelena",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e923",
          "name": "Keqian Li",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e924",
          "name": "Kun Huang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e925",
          "name": "Kunal Chawla",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e926",
          "name": "Kushal Lakhotia",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e927",
          "name": "Kyle Huang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e928",
          "name": "Lailin Chen",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e929",
          "name": "Lakshya Garg",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e92a",
          "name": "Lavender A",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e92b",
          "name": "Leandro Silva",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e92c",
          "name": "Lee Bell",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e92d",
          "name": "Lei Zhang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e92e",
          "name": "Liangpeng Guo",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e92f",
          "name": "Licheng Yu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e930",
          "name": "Liron Moshkovich",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e931",
          "name": "Luca Wehrstedt",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e932",
          "name": "Madian Khabsa",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e933",
          "name": "Manav Avalani",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e934",
          "name": "Manish Bhatt",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e935",
          "name": "Maria Tsimpoukelli",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e936",
          "name": "Martynas Mankus",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e937",
          "name": "Matan Hasson",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e938",
          "name": "Matthew Lennie",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e939",
          "name": "Matthias Reso",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e93a",
          "name": "Maxim Groshev",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e93b",
          "name": "Maxim Naumov",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e93c",
          "name": "Maya Lathi",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e93d",
          "name": "Meghan Keneally",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e93e",
          "name": "Michael L. Seltzer",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e93f",
          "user": {
            "_id": "651e97156d92456bdf5ace6b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e97156d92456bdf5ace6b/QDQN6LXnCkyjRQAp86fwE.jpeg",
            "isPro": false,
            "fullname": "Michal Valko",
            "user": "misovalko",
            "type": "user"
          },
          "name": "Michal Valko",
          "status": "claimed_verified",
          "statusLastChangedAt": "2024-11-15T09:46:17.269Z",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e940",
          "name": "Michelle Restrepo",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e941",
          "name": "Mihir Patel",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e942",
          "name": "Mik Vyatskov",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e943",
          "name": "Mikayel Samvelyan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e944",
          "name": "Mike Clark",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e945",
          "name": "Mike Macey",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e946",
          "name": "Mike Wang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e947",
          "name": "Miquel Jubert Hermoso",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e948",
          "name": "Mo Metanat",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e949",
          "name": "Mohammad Rastegari",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e94a",
          "name": "Munish Bansal",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e94b",
          "name": "Nandhini Santhanam",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e94c",
          "name": "Natascha Parks",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e94d",
          "name": "Natasha White",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e94e",
          "name": "Navyata Bawa",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e94f",
          "name": "Nayan Singhal",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e950",
          "name": "Nick Egebo",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e951",
          "name": "Nicolas Usunier",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e952",
          "name": "Nikolay Pavlovich Laptev",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e953",
          "name": "Ning Dong",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e954",
          "name": "Ning Zhang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e955",
          "name": "Norman Cheng",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e956",
          "name": "Oleg Chernoguz",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e957",
          "name": "Olivia Hart",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e958",
          "name": "Omkar Salpekar",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e959",
          "name": "Ozlem Kalinli",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e95a",
          "name": "Parkin Kent",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e95b",
          "name": "Parth Parekh",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e95c",
          "name": "Paul Saab",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e95d",
          "name": "Pavan Balaji",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e95e",
          "name": "Pedro Rittner",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e95f",
          "name": "Philip Bontrager",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e960",
          "name": "Pierre Roux",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e961",
          "name": "Piotr Dollar",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e962",
          "name": "Polina Zvyagina",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e963",
          "name": "Prashant Ratanchandani",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e964",
          "user": {
            "_id": "65c30d162a023e7ea97c160f",
            "avatarUrl": "/avatars/f17eaa12dd4c4a383fb741f7ae5b9432.svg",
            "isPro": false,
            "fullname": "Pritish Yuvraj",
            "user": "pyuvraj",
            "type": "user"
          },
          "name": "Pritish Yuvraj",
          "status": "claimed_verified",
          "statusLastChangedAt": "2024-10-04T20:11:43.121Z",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e965",
          "name": "Qian Liang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e966",
          "name": "Rachad Alao",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e967",
          "name": "Rachel Rodriguez",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e968",
          "name": "Rafi Ayub",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e969",
          "name": "Raghotham Murthy",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e96a",
          "name": "Raghu Nayani",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e96b",
          "name": "Rahul Mitra",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e96c",
          "name": "Raymond Li",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e96d",
          "name": "Rebekkah Hogan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e96e",
          "name": "Robin Battey",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e96f",
          "name": "Rocky Wang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e970",
          "name": "Rohan Maheswari",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e971",
          "name": "Russ Howes",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e972",
          "name": "Ruty Rinott",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e973",
          "name": "Sai Jayesh Bondu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e974",
          "name": "Samyak Datta",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e975",
          "name": "Sara Chugh",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e976",
          "name": "Sara Hunt",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e977",
          "name": "Sargun Dhillon",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e978",
          "name": "Sasha Sidorov",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e979",
          "name": "Satadru Pan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e97a",
          "name": "Saurabh Verma",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e97b",
          "name": "Seiji Yamamoto",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e97c",
          "name": "Sharadh Ramaswamy",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e97d",
          "name": "Shaun Lindsay",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e97e",
          "name": "Shaun Lindsay",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e97f",
          "name": "Sheng Feng",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e980",
          "name": "Shenghao Lin",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e981",
          "name": "Shengxin Cindy Zha",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e982",
          "name": "Shiva Shankar",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e983",
          "name": "Shuqiang Zhang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e984",
          "name": "Shuqiang Zhang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e985",
          "name": "Sinong Wang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e986",
          "name": "Sneha Agarwal",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e987",
          "name": "Soji Sajuyigbe",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e988",
          "name": "Soumith Chintala",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e989",
          "name": "Stephanie Max",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e98a",
          "name": "Stephen Chen",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e98b",
          "name": "Steve Kehoe",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e98c",
          "name": "Steve Satterfield",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e98d",
          "name": "Sudarshan Govindaprasad",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e98e",
          "name": "Sumit Gupta",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e98f",
          "name": "Sungmin Cho",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e990",
          "name": "Sunny Virk",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e991",
          "name": "Suraj Subramanian",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e992",
          "name": "Sy Choudhury",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e993",
          "name": "Sydney Goldman",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e994",
          "name": "Tal Remez",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e995",
          "name": "Tamar Glaser",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e996",
          "name": "Tamara Best",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e997",
          "name": "Thilo Kohler",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e998",
          "name": "Thomas Robinson",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e999",
          "name": "Tianhe Li",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e99a",
          "user": {
            "_id": "6374cd6b6ea8da14f8fef8dc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6374cd6b6ea8da14f8fef8dc/l13bg0tKDjCnUw3I895QZ.png",
            "isPro": false,
            "fullname": "Tianjun Zhang",
            "user": "tianjunz",
            "type": "user"
          },
          "name": "Tianjun Zhang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2024-08-01T07:00:20.001Z",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e99b",
          "name": "Tim Matthews",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e99c",
          "name": "Timothy Chou",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e99d",
          "name": "Tzook Shaked",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e99e",
          "name": "Varun Vontimitta",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e99f",
          "name": "Victoria Ajayi",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9a0",
          "name": "Victoria Montanez",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9a1",
          "name": "Vijai Mohan",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9a2",
          "name": "Vinay Satish Kumar",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9a3",
          "name": "Vishal Mangla",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9a4",
          "name": "Vlad Ionescu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9a5",
          "name": "Vlad Poenaru",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9a6",
          "name": "Vlad Tiberiu Mihailescu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9a7",
          "name": "Vladimir Ivanov",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9a8",
          "name": "Wei Li",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9a9",
          "name": "Wenchen Wang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9aa",
          "name": "Wenwen Jiang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9ab",
          "name": "Wes Bouaziz",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9ac",
          "name": "Will Constable",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9ad",
          "name": "Xiaocheng Tang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9ae",
          "name": "Xiaofang Wang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9af",
          "name": "Xiaojian Wu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9b0",
          "name": "Xiaolan Wang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9b1",
          "name": "Xide Xia",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9b2",
          "name": "Xilun Wu",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9b3",
          "name": "Xinbo Gao",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9b4",
          "name": "Yanjun Chen",
          "hidden": false
        },
        { "_id": "66aaf835636a86cff6f5e9b5", "name": "Ye Hu", "hidden": false },
        {
          "_id": "66aaf835636a86cff6f5e9b6",
          "name": "Ye Jia",
          "hidden": false
        },
        { "_id": "66aaf835636a86cff6f5e9b7", "name": "Ye Qi", "hidden": false },
        {
          "_id": "66aaf835636a86cff6f5e9b8",
          "name": "Yenda Li",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9b9",
          "name": "Yilin Zhang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9ba",
          "name": "Ying Zhang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9bb",
          "name": "Yossi Adi",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9bc",
          "name": "Youngjin Nam",
          "hidden": false
        },
        { "_id": "66aaf835636a86cff6f5e9bd", "name": "Yu", "hidden": false },
        { "_id": "66aaf835636a86cff6f5e9be", "name": "Wang", "hidden": false },
        {
          "_id": "66aaf835636a86cff6f5e9bf",
          "name": "Yuchen Hao",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9c0",
          "name": "Yundi Qian",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9c1",
          "name": "Yuzi He",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9c2",
          "name": "Zach Rait",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9c3",
          "name": "Zachary DeVito",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9c4",
          "name": "Zef Rosnbrick",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9c5",
          "name": "Zhaoduo Wen",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9c6",
          "name": "Zhenyu Yang",
          "hidden": false
        },
        {
          "_id": "66aaf835636a86cff6f5e9c7",
          "name": "Zhiwei Zhao",
          "hidden": false
        }
      ],
      "publishedAt": "2024-07-31T17:54:27.000Z",
      "title": "The Llama 3 Herd of Models",
      "summary": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
      "upvotes": 115,
      "discussionId": "66aaf837636a86cff6f5ea39",
      "ai_keywords": [
        "Transformer",
        "multilinguality",
        "coding",
        "reasoning",
        "tool usage",
        "context window",
        "empirical evaluation",
        "Llama 3",
        "pre-trained",
        "post-trained",
        "Llama Guard 3",
        "compositional approach",
        "image recognition",
        "video recognition",
        "speech recognition"
      ]
    },
    "publishedAt": "2024-07-31T13:54:27.000Z",
    "title": "The Llama 3 Herd of Models",
    "summary": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.21783.png",
    "numComments": 6,
    "upvoted": false,
    "isAuthorParticipating": true,
    "highlightedTitle": [
      { "type": "text", "text": "The " },
      { "type": "highlight", "text": "Llama" },
      { "type": "text", "text": " " },
      { "type": "highlight", "text": "3" },
      { "type": "text", "text": " " },
      { "type": "highlight", "text": "Herd" },
      { "type": "text", "text": " of Models" }
    ],
    "highlightedSummary": [
      {
        "type": "text",
        "text": "Modern artificial intelligence (AI) systems are powered by foundation models.\nThis paper presents a new set of foundation models, called Llama 3. It is a\nherd of language models that natively support multilinguality, coding,\nreasoning, and tool usage. Our largest model is a dense Transformer with 405B\nparameters and a context window of up to 128K tokens. This paper presents an\nextensive empirical evaluation of Llama 3. We find that Llama 3 delivers\ncomparable quality to leading language models such as GPT-4 on a plethora of\ntasks. We publicly release Llama 3, including pre-trained and post-trained\nversions of the 405B parameter language model and our Llama Guard 3 model for\ninput and output safety. The paper also presents the results of experiments in\nwhich we integrate image, video, and speech capabilities into Llama 3 via a\ncompositional approach. We observe this approach performs competitively with\nthe state-of-the-art on image, video, and speech recognition tasks. The\nresulting models are not yet being broadly released as they are still under\ndevelopment."
      }
    ]
  },
  {
    "paper": {
      "id": "2501.13921",
      "authors": [
        {
          "_id": "6792fd5faed70aeef9518dc4",
          "name": "Chan-Jan Hsu",
          "hidden": false
        },
        {
          "_id": "6792fd5faed70aeef9518dc5",
          "name": "Chia-Sheng Liu",
          "hidden": false
        },
        {
          "_id": "6792fd5faed70aeef9518dc6",
          "name": "Meng-Hsi Chen",
          "hidden": false
        },
        {
          "_id": "6792fd5faed70aeef9518dc7",
          "name": "Muxi Chen",
          "hidden": false
        },
        {
          "_id": "6792fd5faed70aeef9518dc8",
          "name": "Po-Chun Hsu",
          "hidden": false
        },
        {
          "_id": "6792fd5faed70aeef9518dc9",
          "user": {
            "_id": "63808c5d187bda141fbaf2b6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63808c5d187bda141fbaf2b6/BUiegeDjJUM5L6Ra735pM.png",
            "isPro": false,
            "fullname": "Yi-Chang (YC) Chen",
            "user": "YC-Chen",
            "type": "user"
          },
          "name": "Yi-Chang Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-02-04T09:39:45.462Z",
          "hidden": false
        },
        {
          "_id": "6792fd5faed70aeef9518dca",
          "name": "Da-Shan Shiu",
          "hidden": false
        }
      ],
      "publishedAt": "2025-01-23T18:59:02.000Z",
      "title": "The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama\n  with Vision-Aware and Function-Calling Capabilities",
      "summary": "Breeze 2 is a suite of advanced multi-modal language models, available in 3B\nand 8B parameter configurations, specifically designed to enhance Traditional\nChinese language representation. Building upon the Llama 3, Breeze 2 continues\npretraining on an extensive corpus to enhance the linguistic and cultural\nheritage of Traditional Chinese. It incorporates vision-aware capabilities\nthrough a visual encoder and a bridge module, and supports function-calling via\nprompt templates and post-training on function-calling data. The effectiveness\nof Breeze 2 is benchmarked across various tasks, including Taiwan general\nknowledge, instruction-following, long context, function calling, and vision\nunderstanding. Furthermore, we showcase the capabilities of the its 3B model in\na mobile application. We are publicly releasing all Breeze 2 models under the\nLlama 3 Community License.",
      "upvotes": 3,
      "discussionId": "6792fd60aed70aeef9518dfc",
      "ai_keywords": [
        "multi-modal language models",
        "vision-aware capabilities",
        "visual encoder",
        "bridge module",
        "prompt templates",
        "long context",
        "instruction-following",
        "function calling",
        "vision understanding"
      ]
    },
    "publishedAt": "2025-01-23T13:59:02.000Z",
    "title": "The Breeze 2 Herd of Models: Traditional Chinese LLMs Based on Llama\n  with Vision-Aware and Function-Calling Capabilities",
    "summary": "Breeze 2 is a suite of advanced multi-modal language models, available in 3B\nand 8B parameter configurations, specifically designed to enhance Traditional\nChinese language representation. Building upon the Llama 3, Breeze 2 continues\npretraining on an extensive corpus to enhance the linguistic and cultural\nheritage of Traditional Chinese. It incorporates vision-aware capabilities\nthrough a visual encoder and a bridge module, and supports function-calling via\nprompt templates and post-training on function-calling data. The effectiveness\nof Breeze 2 is benchmarked across various tasks, including Taiwan general\nknowledge, instruction-following, long context, function calling, and vision\nunderstanding. Furthermore, we showcase the capabilities of the its 3B model in\na mobile application. We are publicly releasing all Breeze 2 models under the\nLlama 3 Community License.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2501.13921.png",
    "numComments": 0,
    "upvoted": false,
    "isAuthorParticipating": false,
    "highlightedTitle": [
      { "type": "text", "text": "The Breeze 2 " },
      { "type": "highlight", "text": "Herd" },
      {
        "type": "text",
        "text": " of Models: Traditional Chinese LLMs Based on "
      },
      { "type": "highlight", "text": "Llama" },
      {
        "type": "text",
        "text": "\n  with Vision-Aware and Function-Calling Capabilities"
      }
    ],
    "highlightedSummary": [
      {
        "type": "text",
        "text": "Breeze 2 is a suite of advanced multi-modal language models, available in 3B\nand 8B parameter configurations, specifically designed to enhance Traditional\nChinese language representation. Building upon the "
      },
      { "type": "highlight", "text": "Llama" },
      { "type": "text", "text": " " },
      { "type": "highlight", "text": "3" },
      {
        "type": "text",
        "text": ", Breeze 2 continues\npretraining on an extensive corpus to enhance the linguistic and cultural\nheritage of Traditional Chinese. It incorporates vision-aware capabilities\nthrough a visual encoder and a bridge module, and supports function-calling via\nprompt templates and post-training on function-calling data. The effectiveness\nof Breeze 2 is benchmarked across various tasks, including Taiwan general\nknowledge, instruction-following, long context, function calling, and vision\nunderstanding. Furthermore, we showcase the capabilities of the its 3B model in\na mobile application. We are publicly releasing all Breeze 2 models under the\nLlama 3 Community License."
      }
    ]
  },
  {
    "paper": {
      "id": "2411.10414",
      "authors": [
        {
          "_id": "673b5f0be17a2c7a97a81211",
          "user": {
            "_id": "6530a4eebce21215c3999abd",
            "avatarUrl": "/avatars/8b22bcbbad5dd4fe5956378f5f343294.svg",
            "isPro": false,
            "fullname": "Jianfeng Chi",
            "user": "jfchi",
            "type": "user"
          },
          "name": "Jianfeng Chi",
          "status": "extracted_pending",
          "statusLastChangedAt": "2024-11-18T15:36:44.148Z",
          "hidden": false
        },
        {
          "_id": "673b5f0be17a2c7a97a81212",
          "name": "Ujjwal Karn",
          "hidden": false
        },
        {
          "_id": "673b5f0be17a2c7a97a81213",
          "name": "Hongyuan Zhan",
          "hidden": false
        },
        {
          "_id": "673b5f0be17a2c7a97a81214",
          "name": "Eric Smith",
          "hidden": false
        },
        {
          "_id": "673b5f0be17a2c7a97a81215",
          "name": "Javier Rando",
          "hidden": false
        },
        {
          "_id": "673b5f0be17a2c7a97a81216",
          "name": "Yiming Zhang",
          "hidden": false
        },
        {
          "_id": "673b5f0be17a2c7a97a81217",
          "name": "Kate Plawiak",
          "hidden": false
        },
        {
          "_id": "673b5f0be17a2c7a97a81218",
          "name": "Zacharie Delpierre Coudert",
          "hidden": false
        },
        {
          "_id": "673b5f0be17a2c7a97a81219",
          "name": "Kartikeya Upasani",
          "hidden": false
        },
        {
          "_id": "673b5f0be17a2c7a97a8121a",
          "name": "Mahesh Pasupuleti",
          "hidden": false
        }
      ],
      "publishedAt": "2024-11-15T18:34:07.000Z",
      "title": "Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding\n  Conversations",
      "summary": "We introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for\nhuman-AI conversations that involves image understanding: it can be used to\nsafeguard content for both multimodal LLM inputs (prompt classification) and\noutputs (response classification). Unlike the previous text-only Llama Guard\nversions (Inan et al., 2023; Llama Team, 2024b,a), it is specifically designed\nto support image reasoning use cases and is optimized to detect harmful\nmultimodal (text and image) prompts and text responses to these prompts. Llama\nGuard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong\nperformance on the internal benchmarks using the MLCommons taxonomy. We also\ntest its robustness against adversarial attacks. We believe that Llama Guard 3\nVision serves as a good starting point to build more capable and robust content\nmoderation tools for human-AI conversation with multimodal capabilities.",
      "upvotes": 0,
      "discussionId": "673b5f0ce17a2c7a97a8127d",
      "ai_keywords": [
        "multimodal LLM",
        "Llama Guard 3 Vision",
        "image understanding",
        "prompt classification",
        "response classification",
        "inverted index",
        "provideimodal",
        "harmful provideimodal prompts",
        "fine-tuned",
        "Llama 3.2-Vision",
        "benchmark",
        "MLCommons taxonomy",
        "adversarial attacks",
        "content moderation"
      ]
    },
    "publishedAt": "2024-11-15T13:34:07.000Z",
    "title": "Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding\n  Conversations",
    "summary": "We introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for\nhuman-AI conversations that involves image understanding: it can be used to\nsafeguard content for both multimodal LLM inputs (prompt classification) and\noutputs (response classification). Unlike the previous text-only Llama Guard\nversions (Inan et al., 2023; Llama Team, 2024b,a), it is specifically designed\nto support image reasoning use cases and is optimized to detect harmful\nmultimodal (text and image) prompts and text responses to these prompts. Llama\nGuard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong\nperformance on the internal benchmarks using the MLCommons taxonomy. We also\ntest its robustness against adversarial attacks. We believe that Llama Guard 3\nVision serves as a good starting point to build more capable and robust content\nmoderation tools for human-AI conversation with multimodal capabilities.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.10414.png",
    "numComments": 0,
    "upvoted": false,
    "isAuthorParticipating": false,
    "highlightedTitle": [
      {
        "type": "text",
        "text": "Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding\n  Conversations"
      }
    ],
    "highlightedSummary": [
      {
        "type": "text",
        "text": "We introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for\nhuman-AI conversations that involves image understanding: it can be used to\nsafeguard content for both multimodal LLM inputs (prompt classification) and\noutputs (response classification). Unlike the previous text-only Llama Guard\nversions (Inan et al., 2023; Llama Team, 2024b,a), it is specifically designed\nto support image reasoning use cases and is optimized to detect harmful\nmultimodal (text and image) prompts and text responses to these prompts. "
      },
      {
        "type": "highlight",
        "text": "Llama\nGuard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong\nperformance on the internal benchmarks using the MLCommons taxonomy."
      },
      {
        "type": "text",
        "text": " We also\ntest its robustness against adversarial attacks. We believe that Llama Guard 3\nVision serves as a good starting point to build more capable and robust content\nmoderation tools for human-AI conversation with multimodal capabilities."
      }
    ]
  },
  {
    "paper": {
      "id": "2411.09012",
      "authors": [
        {
          "_id": "6738f2e78ad55753069f34c7",
          "user": {
            "_id": "643f1ddce2ea47d170103537",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643f1ddce2ea47d170103537/R-L3kIDO5EahUquruXkUu.jpeg",
            "isPro": false,
            "fullname": "Tijmen de Haan",
            "user": "Tijmen2",
            "type": "user"
          },
          "name": "Tijmen de Haan",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-01-15T16:36:11.697Z",
          "hidden": false
        },
        {
          "_id": "6738f2e78ad55753069f34c8",
          "name": "Yuan-Sen Ting",
          "hidden": false
        },
        {
          "_id": "6738f2e78ad55753069f34c9",
          "name": "Tirthankar Ghosal",
          "hidden": false
        },
        {
          "_id": "6738f2e78ad55753069f34ca",
          "name": "Tuan Dung Nguyen",
          "hidden": false
        },
        {
          "_id": "6738f2e78ad55753069f34cb",
          "name": "Alberto Accomazzi",
          "hidden": false
        },
        {
          "_id": "6738f2e78ad55753069f34cc",
          "name": "Azton Wells",
          "hidden": false
        },
        {
          "_id": "6738f2e78ad55753069f34cd",
          "name": "Nesar Ramachandra",
          "hidden": false
        },
        {
          "_id": "6738f2e78ad55753069f34ce",
          "name": "Rui Pan",
          "hidden": false
        },
        {
          "_id": "6738f2e78ad55753069f34cf",
          "name": "Zechang Sun",
          "hidden": false
        }
      ],
      "publishedAt": "2024-11-13T20:36:02.000Z",
      "title": "AstroMLab 3: Achieving GPT-4o Level Performance in Astronomy with a\n  Specialized 8B-Parameter Large Language Model",
      "summary": "AstroSage-Llama-3.1-8B is a domain-specialized natural-language AI assistant\ntailored for research in astronomy, astrophysics, and cosmology. Trained on the\ncomplete collection of astronomy-related arXiv papers from 2007-2024 along with\nmillions of synthetically-generated question-answer pairs and other\nastronomical literature, AstroSage-Llama-3.1-8B demonstrates remarkable\nproficiency on a wide range of questions. AstroSage-Llama-3.1-8B scores 80.9%\non the AstroMLab-1 benchmark, greatly outperforming all models -- proprietary\nand open-weight -- in the 8-billion parameter class, and performing on par with\nGPT-4o. This achievement demonstrates the potential of domain specialization in\nAI, suggesting that focused training can yield capabilities exceeding those of\nmuch larger, general-purpose models. AstroSage-Llama-3.1-8B is freely\navailable, enabling widespread access to advanced AI capabilities for\nastronomical education and research.",
      "upvotes": 1,
      "discussionId": "6738f2e78ad55753069f3526"
    },
    "publishedAt": "2024-11-13T15:36:02.000Z",
    "title": "AstroMLab 3: Achieving GPT-4o Level Performance in Astronomy with a\n  Specialized 8B-Parameter Large Language Model",
    "summary": "AstroSage-Llama-3.1-8B is a domain-specialized natural-language AI assistant\ntailored for research in astronomy, astrophysics, and cosmology. Trained on the\ncomplete collection of astronomy-related arXiv papers from 2007-2024 along with\nmillions of synthetically-generated question-answer pairs and other\nastronomical literature, AstroSage-Llama-3.1-8B demonstrates remarkable\nproficiency on a wide range of questions. AstroSage-Llama-3.1-8B scores 80.9%\non the AstroMLab-1 benchmark, greatly outperforming all models -- proprietary\nand open-weight -- in the 8-billion parameter class, and performing on par with\nGPT-4o. This achievement demonstrates the potential of domain specialization in\nAI, suggesting that focused training can yield capabilities exceeding those of\nmuch larger, general-purpose models. AstroSage-Llama-3.1-8B is freely\navailable, enabling widespread access to advanced AI capabilities for\nastronomical education and research.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2411.09012.png",
    "numComments": 0,
    "upvoted": false,
    "isAuthorParticipating": false,
    "highlightedTitle": [
      {
        "type": "text",
        "text": "AstroMLab 3: Achieving GPT-4o Level Performance in Astronomy with a\n  Specialized 8B-Parameter Large Language Model"
      }
    ],
    "highlightedSummary": [
      {
        "type": "highlight",
        "text": "AstroSage-Llama-3.1-8B is a domain-specialized natural-language AI assistant\ntailored for research in astronomy, astrophysics, and cosmology."
      },
      {
        "type": "text",
        "text": " Trained on the\ncomplete collection of astronomy-related arXiv papers from 2007-2024 along with\nmillions of synthetically-generated question-answer pairs and other\nastronomical literature, AstroSage-Llama-3.1-8B demonstrates remarkable\nproficiency on a wide range of questions. AstroSage-Llama-3.1-8B scores 80.9%\non the AstroMLab-1 benchmark, greatly outperforming all models -- proprietary\nand open-weight -- in the 8-billion parameter class, and performing on par with\nGPT-4o. This achievement demonstrates the potential of domain specialization in\nAI, suggesting that focused training can yield capabilities exceeding those of\nmuch larger, general-purpose models. AstroSage-Llama-3.1-8B is freely\navailable, enabling widespread access to advanced AI capabilities for\nastronomical education and research."
      }
    ]
  },
  {
    "paper": {
      "id": "2503.05856",
      "authors": [
        {
          "_id": "67d000d9113c8d298a25100c",
          "user": {
            "_id": "6627bb750723599e5c2fdfcf",
            "avatarUrl": "/avatars/391f72a7a9a9c79f8bf87b55000610cd.svg",
            "isPro": false,
            "fullname": "Lorenz Wolf",
            "user": "Llwo",
            "type": "user"
          },
          "name": "Lorenz Wolf",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-11T10:22:08.605Z",
          "hidden": false
        },
        {
          "_id": "67d000d9113c8d298a25100d",
          "name": "Sangwoong Yoon",
          "hidden": false
        },
        {
          "_id": "67d000d9113c8d298a25100e",
          "name": "Ilija Bogunovic",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-07T14:46:39.000Z",
      "title": "This Is Your Doge, If It Please You: Exploring Deception and Robustness\n  in Mixture of LLMs",
      "summary": "Mixture of large language model (LLMs) Agents (MoA) architectures achieve\nstate-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by\nleveraging the collaboration of multiple LLMs at inference time. Despite these\nsuccesses, an evaluation of the safety and reliability of MoA is missing. We\npresent the first comprehensive study of MoA's robustness against deceptive LLM\nagents that deliberately provide misleading responses. We examine factors like\nthe propagation of deceptive information, model size, and information\navailability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the\npopular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of\n49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate\nthat introducing only a single carefully-instructed deceptive agent\ninto the MoA can reduce performance to 37.9%, effectively nullifying all MoA\ngains. On QuALITY, a multiple-choice comprehension task, the impact is also\nsevere, with accuracy plummeting by a staggering 48.5%. Inspired in part by the\nhistorical Doge of Venice voting process, designed to minimize influence and\ndeception, we propose a range of unsupervised defense mechanisms that recover\nmost of the lost performance.",
      "upvotes": 7,
      "discussionId": "67d000da113c8d298a251042",
      "githubRepo": "https://github.com/lorenzflow/robust-moa",
      "ai_keywords": [
        "Mixture of large language model (LLMs) Agents (MoA)",
        "large language model (LLMs)",
        "AlpacaEval 2.0",
        "inference time",
        "deceptive LLM agents",
        "propagation of deceptive information",
        "model size",
        "information availability",
        "LLaMA 3.1-70B",
        "length-controlled Win Rate (LC WR)",
        "unsupervised defense mechanisms",
        "Doge of Venice voting process"
      ]
    },
    "publishedAt": "2025-03-07T09:46:39.000Z",
    "title": "This Is Your Doge, If It Please You: Exploring Deception and Robustness\n  in Mixture of LLMs",
    "summary": "Mixture of large language model (LLMs) Agents (MoA) architectures achieve\nstate-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by\nleveraging the collaboration of multiple LLMs at inference time. Despite these\nsuccesses, an evaluation of the safety and reliability of MoA is missing. We\npresent the first comprehensive study of MoA's robustness against deceptive LLM\nagents that deliberately provide misleading responses. We examine factors like\nthe propagation of deceptive information, model size, and information\navailability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the\npopular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of\n49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate\nthat introducing only a single carefully-instructed deceptive agent\ninto the MoA can reduce performance to 37.9%, effectively nullifying all MoA\ngains. On QuALITY, a multiple-choice comprehension task, the impact is also\nsevere, with accuracy plummeting by a staggering 48.5%. Inspired in part by the\nhistorical Doge of Venice voting process, designed to minimize influence and\ndeception, we propose a range of unsupervised defense mechanisms that recover\nmost of the lost performance.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.05856.png",
    "numComments": 2,
    "upvoted": false,
    "isAuthorParticipating": true,
    "highlightedTitle": [
      {
        "type": "text",
        "text": "This Is Your Doge, If It Please You: Exploring Deception and Robustness\n  in Mixture of LLMs"
      }
    ],
    "highlightedSummary": [
      {
        "type": "text",
        "text": "Mixture of large language model (LLMs) Agents (MoA) architectures achieve\nstate-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by\nleveraging the collaboration of multiple LLMs at inference time. Despite these\nsuccesses, an evaluation of the safety and reliability of MoA is missing. We\npresent the first comprehensive study of MoA's robustness against deceptive LLM\nagents that deliberately provide misleading responses. We examine factors like\nthe propagation of deceptive information, model size, and information\navailability, and uncover critical vulnerabilities. "
      },
      {
        "type": "highlight",
        "text": "On AlpacaEval 2.0, the\npopular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of\n49.2% when coupled with 3-layer MoA (6 LLM agents)."
      },
      {
        "type": "text",
        "text": " However, we demonstrate\nthat introducing only a single carefully-instructed deceptive agent\ninto the MoA can reduce performance to 37.9%, effectively nullifying all MoA\ngains. On QuALITY, a multiple-choice comprehension task, the impact is also\nsevere, with accuracy plummeting by a staggering 48.5%. Inspired in part by the\nhistorical Doge of Venice voting process, designed to minimize influence and\ndeception, we propose a range of unsupervised defense mechanisms that recover\nmost of the lost performance."
      }
    ]
  },
  {
    "paper": {
      "id": "2406.14833",
      "authors": [
        {
          "_id": "667a35b89f78b2d01bd1b5b7",
          "user": {
            "_id": "64e3282156b920ef00cf7d94",
            "avatarUrl": "/avatars/8a76c27fe6c71eb98a28b4ebfb90336d.svg",
            "isPro": false,
            "fullname": "Guo",
            "user": "YiDuo1999",
            "type": "user"
          },
          "name": "Yiduo Guo",
          "status": "admin_assigned",
          "statusLastChangedAt": "2024-06-25T08:28:43.723Z",
          "hidden": false
        },
        {
          "_id": "667a35b89f78b2d01bd1b5b8",
          "user": {
            "_id": "641a6895fb5ffff5ac79d593",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641a6895fb5ffff5ac79d593/vxvwsto3llOEWGqQKGMYx.jpeg",
            "isPro": false,
            "fullname": "Jie Fu",
            "user": "bigaidream",
            "type": "user"
          },
          "name": "Jie Fu",
          "status": "claimed_verified",
          "statusLastChangedAt": "2024-06-25T08:58:55.208Z",
          "hidden": false
        },
        {
          "_id": "667a35b89f78b2d01bd1b5b9",
          "name": "Huishuai Zhang",
          "hidden": false
        },
        {
          "_id": "667a35b89f78b2d01bd1b5ba",
          "name": "Dongyan Zhao",
          "hidden": false
        },
        {
          "_id": "667a35b89f78b2d01bd1b5bb",
          "user": {
            "_id": "6463995939359568c63d740c",
            "avatarUrl": "/avatars/11caacefc68c1259dbc8764cc4340481.svg",
            "isPro": false,
            "fullname": "Yikang Shen",
            "user": "YikangS",
            "type": "user"
          },
          "name": "Yikang Shen",
          "status": "extracted_confirmed",
          "statusLastChangedAt": "2024-06-25T18:40:38.691Z",
          "hidden": false
        }
      ],
      "publishedAt": "2024-06-21T02:28:37.000Z",
      "title": "Efficient Continual Pre-training by Mitigating the Stability Gap",
      "summary": "Continual pre-training has increasingly become the predominant approach for\nadapting Large Language Models (LLMs) to new domains. This process involves\nupdating the pre-trained LLM with a corpus from a new domain, resulting in a\nshift in the training distribution. To study the behavior of LLMs during this\nshift, we measured the model's performance throughout the continual\npre-training process. we observed a temporary performance drop at the\nbeginning, followed by a recovery phase, a phenomenon known as the \"stability\ngap,\" previously noted in vision models classifying new classes. To address\nthis issue and enhance LLM performance within a fixed compute budget, we\npropose three effective strategies: (1) Continually pre-training the LLM on a\nsubset with a proper size for multiple epochs, resulting in faster performance\nrecovery than pre-training the LLM on a large corpus in a single epoch; (2)\nPre-training the LLM only on high-quality sub-corpus, which rapidly boosts\ndomain performance; and (3) Using a data mixture similar to the pre-training\ndata to reduce distribution gap. We conduct various experiments on Llama-family\nmodels to validate the effectiveness of our strategies in both medical\ncontinual pre-training and instruction tuning. For example, our strategies\nimprove the average medical task performance of the OpenLlama-3B model from\n36.2% to 40.7% with only 40% of the original training budget and enhance the\naverage general task performance without causing forgetting. Furthermore, we\napply our strategies to the Llama-3-8B model. The resulting model,\nLlama-3-Physician, achieves the best medical performance among current\nopen-source models, and performs comparably to or even better than GPT-4 on\nseveral medical benchmarks. We release our models at\nhttps://huggingface.co/YiDuo1999/Llama-3-Physician-8B-Instruct.",
      "upvotes": 20,
      "discussionId": "667a35b99f78b2d01bd1b633",
      "ai_keywords": [
        "Continual pre-training",
        "Large Language Models (LLMs)",
        "Stability gap",
        "Sub-corpus",
        "Epochs",
        "High-quality sub-corpus",
        "Data mixture",
        "Llama-family models",
        "OpenLlama-3B",
        "Medical task performance",
        "Llama-3-8B",
        "Llama-3-Physician",
        "GPT-4",
        "Medical benchmarks"
      ]
    },
    "publishedAt": "2024-06-20T22:28:37.000Z",
    "title": "Efficient Continual Pre-training by Mitigating the Stability Gap",
    "summary": "Continual pre-training has increasingly become the predominant approach for\nadapting Large Language Models (LLMs) to new domains. This process involves\nupdating the pre-trained LLM with a corpus from a new domain, resulting in a\nshift in the training distribution. To study the behavior of LLMs during this\nshift, we measured the model's performance throughout the continual\npre-training process. we observed a temporary performance drop at the\nbeginning, followed by a recovery phase, a phenomenon known as the \"stability\ngap,\" previously noted in vision models classifying new classes. To address\nthis issue and enhance LLM performance within a fixed compute budget, we\npropose three effective strategies: (1) Continually pre-training the LLM on a\nsubset with a proper size for multiple epochs, resulting in faster performance\nrecovery than pre-training the LLM on a large corpus in a single epoch; (2)\nPre-training the LLM only on high-quality sub-corpus, which rapidly boosts\ndomain performance; and (3) Using a data mixture similar to the pre-training\ndata to reduce distribution gap. We conduct various experiments on Llama-family\nmodels to validate the effectiveness of our strategies in both medical\ncontinual pre-training and instruction tuning. For example, our strategies\nimprove the average medical task performance of the OpenLlama-3B model from\n36.2% to 40.7% with only 40% of the original training budget and enhance the\naverage general task performance without causing forgetting. Furthermore, we\napply our strategies to the Llama-3-8B model. The resulting model,\nLlama-3-Physician, achieves the best medical performance among current\nopen-source models, and performs comparably to or even better than GPT-4 on\nseveral medical benchmarks. We release our models at\nhttps://huggingface.co/YiDuo1999/Llama-3-Physician-8B-Instruct.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2406.14833.png",
    "numComments": 1,
    "upvoted": false,
    "isAuthorParticipating": true,
    "highlightedTitle": [
      {
        "type": "text",
        "text": "Efficient Continual Pre-training by Mitigating the Stability Gap"
      }
    ],
    "highlightedSummary": [
      {
        "type": "text",
        "text": "Continual pre-training has increasingly become the predominant approach for\nadapting Large Language Models (LLMs) to new domains. This process involves\nupdating the pre-trained LLM with a corpus from a new domain, resulting in a\nshift in the training distribution. To study the behavior of LLMs during this\nshift, we measured the model's performance throughout the continual\npre-training process. we observed a temporary performance drop at the\nbeginning, followed by a recovery phase, a phenomenon known as the \"stability\ngap,\" previously noted in vision models classifying new classes. To address\nthis issue and enhance LLM performance within a fixed compute budget, we\npropose three effective strategies: (1) Continually pre-training the LLM on a\nsubset with a proper size for multiple epochs, resulting in faster performance\nrecovery than pre-training the LLM on a large corpus in a single epoch; (2)\nPre-training the LLM only on high-quality sub-corpus, which rapidly boosts\ndomain performance; and (3) Using a data mixture similar to the pre-training\ndata to reduce distribution gap. We conduct various experiments on Llama-family\nmodels to validate the effectiveness of our strategies in both medical\ncontinual pre-training and instruction tuning. For example, our strategies\nimprove the average medical task performance of the OpenLlama-3B model from\n36.2% to 40.7% with only 40% of the original training budget and enhance the\naverage general task performance without causing forgetting. Furthermore, we\napply our strategies to the Llama-3-8B model. "
      },
      {
        "type": "highlight",
        "text": "The resulting model,\nLlama-3-Physician, achieves the best medical performance among current\nopen-source models, and performs comparably to or even better than GPT-4 on\nseveral medical benchmarks."
      },
      {
        "type": "text",
        "text": " We release our models at\nhttps://huggingface.co/YiDuo1999/Llama-3-Physician-8B-Instruct."
      }
    ]
  },
  {
    "paper": {
      "id": "2302.13971",
      "authors": [
        {
          "_id": "6411c7826b75ddced3891cae",
          "name": "Hugo Touvron",
          "hidden": false
        },
        {
          "_id": "6411c7826b75ddced3891caf",
          "name": "Thibaut Lavril",
          "hidden": false
        },
        {
          "_id": "6411c7826b75ddced3891cb0",
          "user": {
            "_id": "61e6de7282b19b93e1a51fa7",
            "avatarUrl": "/avatars/96b883c802e441410e90b3bec9381d44.svg",
            "isPro": false,
            "fullname": "Gautier Izacard",
            "user": "gizacard",
            "type": "user"
          },
          "name": "Gautier Izacard",
          "status": "admin_assigned",
          "statusLastChangedAt": "2023-07-19T10:06:12.668Z",
          "hidden": false
        },
        {
          "_id": "6411c7826b75ddced3891cb1",
          "user": {
            "_id": "642c2e4efa6e17ba1e29c50c",
            "avatarUrl": "/avatars/7a7aa234558f7ef3fb99a6f86d19bee9.svg",
            "isPro": false,
            "fullname": "Xavier Martinet",
            "user": "javier-m",
            "type": "user"
          },
          "name": "Xavier Martinet",
          "status": "admin_assigned",
          "statusLastChangedAt": "2023-04-04T14:05:04.046Z",
          "hidden": false
        },
        {
          "_id": "6411c7826b75ddced3891cb2",
          "name": "Marie-Anne Lachaux",
          "hidden": false
        },
        {
          "_id": "6411c7826b75ddced3891cb3",
          "name": "Timothée Lacroix",
          "hidden": false
        },
        {
          "_id": "6411c7826b75ddced3891cb4",
          "name": "Baptiste Rozière",
          "hidden": false
        },
        {
          "_id": "6411c7826b75ddced3891cb5",
          "name": "Naman Goyal",
          "hidden": false
        },
        {
          "_id": "6411c7826b75ddced3891cb6",
          "name": "Eric Hambro",
          "hidden": false
        },
        {
          "_id": "6411c7826b75ddced3891cb7",
          "name": "Faisal Azhar",
          "hidden": false
        },
        {
          "_id": "6411c7826b75ddced3891cb8",
          "name": "Aurelien Rodriguez",
          "hidden": false
        },
        {
          "_id": "6411c7826b75ddced3891cb9",
          "name": "Armand Joulin",
          "hidden": false
        },
        {
          "_id": "6411c7826b75ddced3891cba",
          "name": "Edouard Grave",
          "hidden": false
        },
        {
          "_id": "6411c7826b75ddced3891cbb",
          "name": "Guillaume Lample",
          "hidden": false
        }
      ],
      "publishedAt": "2023-02-27T17:11:15.000Z",
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "summary": "We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We train our models on trillions of tokens, and show that\nit is possible to train state-of-the-art models using publicly available\ndatasets exclusively, without resorting to proprietary and inaccessible\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\nPaLM-540B. We release all our models to the research community.",
      "upvotes": 14,
      "discussionId": "6411923a3ea54b1aa7e2f9a1",
      "ai_keywords": [
        "foundation language models",
        "trillions of tokens",
        "state-of-the-art models",
        "publicly available datasets",
        "proprietary and inaccessible datasets",
        "LLaMA-13B",
        "GPT-3",
        "LLaMA-65B",
        "Chinchilla-70B",
        "PaLM-540B"
      ]
    },
    "publishedAt": "2023-02-27T12:11:15.000Z",
    "title": "LLaMA: Open and Efficient Foundation Language Models",
    "summary": "We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We train our models on trillions of tokens, and show that\nit is possible to train state-of-the-art models using publicly available\ndatasets exclusively, without resorting to proprietary and inaccessible\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\nPaLM-540B. We release all our models to the research community.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2302.13971.png",
    "numComments": 8,
    "upvoted": false,
    "isAuthorParticipating": false,
    "highlightedTitle": [
      {
        "type": "text",
        "text": "LLaMA: Open and Efficient Foundation Language Models"
      }
    ],
    "highlightedSummary": [
      {
        "type": "text",
        "text": "We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We train our models on trillions of tokens, and show that\nit is possible to train state-of-the-art models using publicly available\ndatasets exclusively, without resorting to proprietary and inaccessible\ndatasets. "
      },
      {
        "type": "highlight",
        "text": "In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\nPaLM-540B."
      },
      {
        "type": "text",
        "text": " We release all our models to the research community."
      }
    ]
  },
  {
    "paper": {
      "id": "2408.14774",
      "authors": [
        {
          "_id": "66df08af1b86ce2e10d8d4c2",
          "name": "Simran Kaur",
          "hidden": false
        },
        {
          "_id": "66df08af1b86ce2e10d8d4c3",
          "name": "Simon Park",
          "hidden": false
        },
        {
          "_id": "66df08af1b86ce2e10d8d4c4",
          "name": "Anirudh Goyal",
          "hidden": false
        },
        {
          "_id": "66df08af1b86ce2e10d8d4c5",
          "name": "Sanjeev Arora",
          "hidden": false
        }
      ],
      "publishedAt": "2024-08-27T04:31:58.000Z",
      "title": "Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning",
      "summary": "We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just 4K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in 20% of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings.",
      "upvotes": 0,
      "discussionId": "66df08b01b86ce2e10d8d4f7",
      "ai_keywords": [
        "LLM",
        "Skill extraction",
        "Data generation",
        "Instruction-following",
        "SFT (Supervised Fine-Tuning)",
        "PPO",
        "DPO",
        "RL (Reinforcement Learning)",
        "AlpacaEval 2.0",
        "MT-Bench",
        "WildBench",
        "length-controlled win rate",
        "Claude 3 Opus"
      ]
    },
    "publishedAt": "2024-08-27T00:31:58.000Z",
    "title": "Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning",
    "summary": "We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just 4K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in 20% of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2408.14774.png",
    "numComments": 0,
    "upvoted": false,
    "isAuthorParticipating": false,
    "highlightedTitle": [
      {
        "type": "text",
        "text": "Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning"
      }
    ],
    "highlightedSummary": [
      {
        "type": "text",
        "text": "We introduce Instruct-SkillMix, an automated approach for creating diverse,\nhigh quality SFT data. The Instruct-SkillMix pipeline involves two stages, each\nleveraging an existing powerful LLM: (1) Skill extraction: uses the LLM to\nextract core \"skills\" for instruction-following, either from existing datasets,\nor by directly prompting the model; (2) Data generation: uses the powerful LLM\nto generate (instruction, response) data that exhibit a randomly chosen pair of\nthese skills. Here, the use of random skill combinations promotes diversity and\ndifficulty.\n  Vanilla SFT (i.e., no PPO, DPO, or RL methods) on data generated from\nInstruct-SkillMix leads to strong gains on instruction following benchmarks\nsuch as AlpacaEval 2.0, MT-Bench, and WildBench. With just 4K examples,\nLLaMA-3-8B-Base achieves 42.76% length-controlled win rate on AlpacaEval 2.0.\nTo our knowledge, this achieves state-of-the-art performance among all models\nthat have only undergone SFT (no RL methods) and competes with proprietary\nmodels such as Claude 3 Opus and LLaMA-3.1-405B-Instruct.\n  Ablation studies also suggest plausible reasons for why creating open\ninstruction-tuning datasets via naive crowd-sourcing has proved difficult.\nIntroducing low quality answers (\"shirkers\") in 20% of Instruct-SkillMix\nexamples causes performance to plummet, sometimes catastrophically.\n  The Instruct-SkillMix pipeline is flexible and is adaptable to other\nsettings."
      }
    ]
  },
  {
    "paper": {
      "id": "2503.01493",
      "authors": [
        {
          "_id": "67c7e45823ded64a09851f7d",
          "name": "Fajri Koto",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f7e",
          "name": "Rituraj Joshi",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f7f",
          "name": "Nurdaulet Mukhituly",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f80",
          "name": "Yuxia Wang",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f81",
          "name": "Zhuohan Xie",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f82",
          "name": "Rahul Pal",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f83",
          "name": "Daniil Orel",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f84",
          "name": "Parvez Mullah",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f85",
          "name": "Diana Turmakhan",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f86",
          "name": "Maiya Goloburda",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f87",
          "name": "Mohammed Kamran",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f88",
          "name": "Samujjwal Ghosh",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f89",
          "name": "Bokang Jia",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f8a",
          "name": "Jonibek Mansurov",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f8b",
          "name": "Mukhammed Togmanov",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f8c",
          "name": "Debopriyo Banerjee",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f8d",
          "name": "Nurkhan Laiyk",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f8e",
          "name": "Akhmed Sakip",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f8f",
          "name": "Xudong Han",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f90",
          "name": "Ekaterina Kochmar",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f91",
          "name": "Alham Fikri Aji",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f92",
          "name": "Aaryamonvikram Singh",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f93",
          "name": "Alok Anil Jadhav",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f94",
          "name": "Satheesh Katipomu",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f95",
          "name": "Samta Kamboj",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f96",
          "name": "Monojit Choudhury",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f97",
          "name": "Gurpreet Gosal",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f98",
          "name": "Gokul Ramakrishnan",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f99",
          "name": "Biswajit Mishra",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f9a",
          "name": "Sarath Chandran",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f9b",
          "name": "Avraham Sheinin",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f9c",
          "name": "Natalia Vassilieva",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f9d",
          "name": "Neha Sengupta",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f9e",
          "name": "Larry Murray",
          "hidden": false
        },
        {
          "_id": "67c7e45823ded64a09851f9f",
          "name": "Preslav Nakov",
          "hidden": false
        }
      ],
      "publishedAt": "2025-03-03T13:05:48.000Z",
      "title": "Llama-3.1-Sherkala-8B-Chat: An Open Large Language Model for Kazakh",
      "summary": "Llama-3.1-Sherkala-8B-Chat, or Sherkala-Chat (8B) for short, is a\nstate-of-the-art instruction-tuned open generative large language model (LLM)\ndesigned for Kazakh. Sherkala-Chat (8B) aims to enhance the inclusivity of LLM\nadvancements for Kazakh speakers. Adapted from the LLaMA-3.1-8B model,\nSherkala-Chat (8B) is trained on 45.3B tokens across Kazakh, English, Russian,\nand Turkish. With 8 billion parameters, it demonstrates strong knowledge and\nreasoning abilities in Kazakh, significantly outperforming existing open Kazakh\nand multilingual models of similar scale while achieving competitive\nperformance in English. We release Sherkala-Chat (8B) as an open-weight\ninstruction-tuned model and provide a detailed overview of its training,\nfine-tuning, safety alignment, and evaluation, aiming to advance research and\nsupport diverse real-world applications.",
      "upvotes": 1,
      "discussionId": "67c7e45e23ded64a09852231",
      "ai_keywords": [
        "instruction-tuned",
        "open generative large language model",
        "LLaMA-3.1-8B",
        "token",
        "parameter-efficient fine-tuning",
        "safety alignment"
      ]
    },
    "publishedAt": "2025-03-03T08:05:48.000Z",
    "title": "Llama-3.1-Sherkala-8B-Chat: An Open Large Language Model for Kazakh",
    "summary": "Llama-3.1-Sherkala-8B-Chat, or Sherkala-Chat (8B) for short, is a\nstate-of-the-art instruction-tuned open generative large language model (LLM)\ndesigned for Kazakh. Sherkala-Chat (8B) aims to enhance the inclusivity of LLM\nadvancements for Kazakh speakers. Adapted from the LLaMA-3.1-8B model,\nSherkala-Chat (8B) is trained on 45.3B tokens across Kazakh, English, Russian,\nand Turkish. With 8 billion parameters, it demonstrates strong knowledge and\nreasoning abilities in Kazakh, significantly outperforming existing open Kazakh\nand multilingual models of similar scale while achieving competitive\nperformance in English. We release Sherkala-Chat (8B) as an open-weight\ninstruction-tuned model and provide a detailed overview of its training,\nfine-tuning, safety alignment, and evaluation, aiming to advance research and\nsupport diverse real-world applications.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2503.01493.png",
    "numComments": 0,
    "upvoted": false,
    "isAuthorParticipating": false,
    "highlightedTitle": [
      {
        "type": "text",
        "text": "Llama-3.1-Sherkala-8B-Chat: An Open Large Language Model for Kazakh"
      }
    ],
    "highlightedSummary": [
      {
        "type": "highlight",
        "text": "Llama-3.1-Sherkala-8B-Chat, or Sherkala-Chat (8B) for short, is a\nstate-of-the-art instruction-tuned open generative large language model (LLM)\ndesigned for Kazakh."
      },
      {
        "type": "text",
        "text": " Sherkala-Chat (8B) aims to enhance the inclusivity of LLM\nadvancements for Kazakh speakers. Adapted from the LLaMA-3.1-8B model,\nSherkala-Chat (8B) is trained on 45.3B tokens across Kazakh, English, Russian,\nand Turkish. With 8 billion parameters, it demonstrates strong knowledge and\nreasoning abilities in Kazakh, significantly outperforming existing open Kazakh\nand multilingual models of similar scale while achieving competitive\nperformance in English. We release Sherkala-Chat (8B) as an open-weight\ninstruction-tuned model and provide a detailed overview of its training,\nfine-tuning, safety alignment, and evaluation, aiming to advance research and\nsupport diverse real-world applications."
      }
    ]
  },
  {
    "paper": {
      "id": "2404.14047",
      "authors": [
        {
          "_id": "662712bd97eadbff167d0c78",
          "user": {
            "_id": "656db3f53dc1d277e5a64410",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656db3f53dc1d277e5a64410/9kiY2K3MCRcBDk7MrkTBK.png",
            "isPro": false,
            "fullname": "Wei Huang",
            "user": "AaronHuangWei",
            "type": "user"
          },
          "name": "Wei Huang",
          "status": "claimed_verified",
          "statusLastChangedAt": "2024-04-23T07:15:11.751Z",
          "hidden": false
        },
        {
          "_id": "662712bd97eadbff167d0c79",
          "user": {
            "_id": "640df5f38512ec51d7f1f53b",
            "avatarUrl": "/avatars/f3d049b3a7db98437978f8646741ebe4.svg",
            "isPro": false,
            "fullname": "Xudong Ma",
            "user": "Macaronlin",
            "type": "user"
          },
          "name": "Xudong Ma",
          "status": "admin_assigned",
          "statusLastChangedAt": "2024-04-23T07:59:31.991Z",
          "hidden": false
        },
        {
          "_id": "662712bd97eadbff167d0c7a",
          "user": {
            "_id": "65c49589c0b1921e19260a8d",
            "avatarUrl": "/avatars/7ce9af8c627f2a0c3db6bde82290ee1f.svg",
            "isPro": false,
            "fullname": "Haotong Qin",
            "user": "HaotongQin",
            "type": "user"
          },
          "name": "Haotong Qin",
          "status": "admin_assigned",
          "statusLastChangedAt": "2024-04-23T07:59:38.224Z",
          "hidden": false
        },
        {
          "_id": "662712bd97eadbff167d0c7b",
          "user": {
            "_id": "64612660933afb0106a9dee3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Ea83e0zR_2m8foWy6J0AF.jpeg",
            "isPro": false,
            "fullname": "Xingyu Zheng",
            "user": "Xingyu-Zheng",
            "type": "user"
          },
          "name": "Xingyu Zheng",
          "status": "admin_assigned",
          "statusLastChangedAt": "2024-04-23T07:59:44.716Z",
          "hidden": false
        },
        {
          "_id": "662712bd97eadbff167d0c7c",
          "user": {
            "_id": "64e9bfc3f494f8b2a061a010",
            "avatarUrl": "/avatars/e55cfea55b45b03d1abfa38db6af58b6.svg",
            "isPro": false,
            "fullname": "吕呈滔",
            "user": "lvchengtao",
            "type": "user"
          },
          "name": "Chengtao Lv",
          "status": "admin_assigned",
          "statusLastChangedAt": "2024-04-23T07:59:57.501Z",
          "hidden": false
        },
        {
          "_id": "662712bd97eadbff167d0c7d",
          "user": {
            "_id": "64f880358a4c5c401d881ac4",
            "avatarUrl": "/avatars/a50fff2386b4427a081ce26d9f877462.svg",
            "isPro": false,
            "fullname": "ChenHong",
            "user": "chhhhhhh",
            "type": "user"
          },
          "name": "Hong Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2024-07-23T07:52:03.496Z",
          "hidden": false
        },
        {
          "_id": "662712bd97eadbff167d0c7e",
          "name": "Jie Luo",
          "hidden": false
        },
        {
          "_id": "662712bd97eadbff167d0c7f",
          "name": "Xiaojuan Qi",
          "hidden": false
        },
        {
          "_id": "662712bd97eadbff167d0c80",
          "name": "Xianglong Liu",
          "hidden": false
        },
        {
          "_id": "662712bd97eadbff167d0c81",
          "name": "Michele Magno",
          "hidden": false
        }
      ],
      "publishedAt": "2024-04-22T10:03:03.000Z",
      "title": "How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study",
      "summary": "Meta's LLaMA family has become one of the most powerful open-source Large\nLanguage Model (LLM) series. Notably, LLaMA3 models have recently been released\nand achieve impressive performance across various with super-large scale\npre-training on over 15T tokens of data. Given the wide application of low-bit\nquantization for LLMs in resource-limited scenarios, we explore LLaMA3's\ncapabilities when quantized to low bit-width. This exploration holds the\npotential to unveil new insights and challenges for low-bit quantization of\nLLaMA3 and other forthcoming LLMs, especially in addressing performance\ndegradation problems that suffer in LLM compression. Specifically, we evaluate\nthe 10 existing post-training quantization and LoRA-finetuning methods of\nLLaMA3 on 1-8 bits and diverse datasets to comprehensively reveal LLaMA3's\nlow-bit quantization performance. Our experiment results indicate that LLaMA3\nstill suffers non-negligent degradation in these scenarios, especially in\nultra-low bit-width. This highlights the significant performance gap under low\nbit-width that needs to be bridged in future developments. We expect that this\nempirical study will prove valuable in advancing future models, pushing the\nLLMs to lower bit-width with higher accuracy for being practical. Our project\nis released on https://github.com/Macaronlin/LLaMA3-Quantization and quantized\nLLaMA3 models are released in https://huggingface.co/LLMQ.",
      "upvotes": 46,
      "discussionId": "662712bf97eadbff167d0cbf",
      "ai_keywords": [
        "low-bit quantization",
        "ultra-low bit-width",
        "post-training quantization",
        "LoRA-finetuning",
        "LLaMA3"
      ]
    },
    "publishedAt": "2024-04-22T06:03:03.000Z",
    "title": "How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study",
    "summary": "Meta's LLaMA family has become one of the most powerful open-source Large\nLanguage Model (LLM) series. Notably, LLaMA3 models have recently been released\nand achieve impressive performance across various with super-large scale\npre-training on over 15T tokens of data. Given the wide application of low-bit\nquantization for LLMs in resource-limited scenarios, we explore LLaMA3's\ncapabilities when quantized to low bit-width. This exploration holds the\npotential to unveil new insights and challenges for low-bit quantization of\nLLaMA3 and other forthcoming LLMs, especially in addressing performance\ndegradation problems that suffer in LLM compression. Specifically, we evaluate\nthe 10 existing post-training quantization and LoRA-finetuning methods of\nLLaMA3 on 1-8 bits and diverse datasets to comprehensively reveal LLaMA3's\nlow-bit quantization performance. Our experiment results indicate that LLaMA3\nstill suffers non-negligent degradation in these scenarios, especially in\nultra-low bit-width. This highlights the significant performance gap under low\nbit-width that needs to be bridged in future developments. We expect that this\nempirical study will prove valuable in advancing future models, pushing the\nLLMs to lower bit-width with higher accuracy for being practical. Our project\nis released on https://github.com/Macaronlin/LLaMA3-Quantization and quantized\nLLaMA3 models are released in https://huggingface.co/LLMQ.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2404.14047.png",
    "numComments": 12,
    "upvoted": false,
    "isAuthorParticipating": true,
    "highlightedTitle": [
      {
        "type": "text",
        "text": "How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study"
      }
    ],
    "highlightedSummary": [
      {
        "type": "highlight",
        "text": "Meta's LLaMA family has become one of the most powerful open-source Large\nLanguage Model (LLM) series."
      },
      {
        "type": "text",
        "text": " Notably, LLaMA3 models have recently been released\nand achieve impressive performance across various with super-large scale\npre-training on over 15T tokens of data. Given the wide application of low-bit\nquantization for LLMs in resource-limited scenarios, we explore LLaMA3's\ncapabilities when quantized to low bit-width. This exploration holds the\npotential to unveil new insights and challenges for low-bit quantization of\nLLaMA3 and other forthcoming LLMs, especially in addressing performance\ndegradation problems that suffer in LLM compression. Specifically, we evaluate\nthe 10 existing post-training quantization and LoRA-finetuning methods of\nLLaMA3 on 1-8 bits and diverse datasets to comprehensively reveal LLaMA3's\nlow-bit quantization performance. Our experiment results indicate that LLaMA3\nstill suffers non-negligent degradation in these scenarios, especially in\nultra-low bit-width. This highlights the significant performance gap under low\nbit-width that needs to be bridged in future developments. We expect that this\nempirical study will prove valuable in advancing future models, pushing the\nLLMs to lower bit-width with higher accuracy for being practical. Our project\nis released on https://github.com/Macaronlin/LLaMA3-Quantization and quantized\nLLaMA3 models are released in https://huggingface.co/LLMQ."
      }
    ]
  },
  {
    "paper": {
      "id": "2407.18743",
      "authors": [
        {
          "_id": "66b84ab38016acd77e14d8c7",
          "user": {
            "_id": "651a29d566e78720a78317ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651a29d566e78720a78317ec/agx5hmgWBvnsEcsCPXec7.jpeg",
            "isPro": false,
            "fullname": "Jie Chen",
            "user": "survivi",
            "type": "user"
          },
          "name": "Jie Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T12:46:45.237Z",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8c8",
          "user": {
            "_id": "629b765ce1af194c641fcbc6",
            "avatarUrl": "/avatars/7c53a4c2a1e528c19641a2b601731754.svg",
            "isPro": false,
            "fullname": "Zhipeng Chen",
            "user": "TimothyCzp",
            "type": "user"
          },
          "name": "Zhipeng Chen",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-03-10T11:09:11.251Z",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8c9",
          "name": "Jiapeng Wang",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8ca",
          "user": {
            "_id": "66f4fcbc29b10ae4c990a2e0",
            "avatarUrl": "/avatars/5d6df3aa5792a031c28274d428c46d84.svg",
            "isPro": false,
            "fullname": "Kun Zhou",
            "user": "FrancisKunZhou",
            "type": "user"
          },
          "name": "Kun Zhou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2025-04-23T08:39:45.679Z",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8cb",
          "name": "Yutao Zhu",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8cc",
          "name": "Jinhao Jiang",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8cd",
          "name": "Yingqian Min",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8ce",
          "name": "Wayne Xin Zhao",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8cf",
          "user": {
            "_id": "66f0bf59e9d50ec57febf751",
            "avatarUrl": "/avatars/be97941e60064e5dd806c6fe9db3c537.svg",
            "isPro": false,
            "fullname": "Zhicheng Dou",
            "user": "douzc",
            "type": "user"
          },
          "name": "Zhicheng Dou",
          "status": "claimed_verified",
          "statusLastChangedAt": "2024-09-23T06:58:41.894Z",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8d0",
          "name": "Jiaxin Mao",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8d1",
          "name": "Yankai Lin",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8d2",
          "name": "Ruihua Song",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8d3",
          "name": "Jun Xu",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8d4",
          "name": "Xu Chen",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8d5",
          "name": "Rui Yan",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8d6",
          "name": "Zhewei Wei",
          "hidden": false
        },
        { "_id": "66b84ab38016acd77e14d8d7", "name": "Di Hu", "hidden": false },
        {
          "_id": "66b84ab38016acd77e14d8d8",
          "name": "Wenbing Huang",
          "hidden": false
        },
        {
          "_id": "66b84ab38016acd77e14d8d9",
          "name": "Ji-Rong Wen",
          "hidden": false
        }
      ],
      "publishedAt": "2024-07-26T13:55:21.000Z",
      "title": "Towards Effective and Efficient Continual Pre-training of Large Language\n  Models",
      "summary": "Continual pre-training (CPT) has been an important approach for adapting\nlanguage models to specific domains or tasks. To make the CPT approach more\ntraceable, this paper presents a technical report for continually pre-training\nLlama-3 (8B), which significantly enhances the Chinese language ability and\nscientific reasoning ability of the backbone model. To enhance the new\nabilities while retaining the original abilities, we design specific data\nmixture and curriculum strategies by utilizing existing datasets and\nsynthesizing high-quality datasets. Specifically, we synthesize\nmultidisciplinary scientific question and answer (QA) pairs based on related\nweb pages, and subsequently incorporate these synthetic data to improve the\nscientific reasoning ability of Llama-3. We refer to the model after CPT as\nLlama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning\nexperiments with a relatively small model -- TinyLlama, and employ the derived\nfindings to train the backbone model. Extensive experiments on a number of\nevaluation benchmarks show that our approach can largely improve the\nperformance of the backbone models, including both the general abilities (+8.81\non C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on\nMATH and +4.13 on SciEval), without hurting the original capacities. Our model,\ndata, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.",
      "upvotes": 0,
      "discussionId": "66b84ab48016acd77e14d91f",
      "ai_keywords": [
        "continual pre-training (CPT)",
        "data mixture",
        "curriculum strategies",
        "multidisciplinary scientific question and answer (QA) pairs",
        "synthetic data enhanced Llama-3 (Llama-3-SynE)",
        "tuning experiments",
        "evaluation benchmarks",
        "C-Eval",
        "CMMLU",
        "MATH",
        "SciEval"
      ]
    },
    "publishedAt": "2024-07-26T09:55:21.000Z",
    "title": "Towards Effective and Efficient Continual Pre-training of Large Language\n  Models",
    "summary": "Continual pre-training (CPT) has been an important approach for adapting\nlanguage models to specific domains or tasks. To make the CPT approach more\ntraceable, this paper presents a technical report for continually pre-training\nLlama-3 (8B), which significantly enhances the Chinese language ability and\nscientific reasoning ability of the backbone model. To enhance the new\nabilities while retaining the original abilities, we design specific data\nmixture and curriculum strategies by utilizing existing datasets and\nsynthesizing high-quality datasets. Specifically, we synthesize\nmultidisciplinary scientific question and answer (QA) pairs based on related\nweb pages, and subsequently incorporate these synthetic data to improve the\nscientific reasoning ability of Llama-3. We refer to the model after CPT as\nLlama-3-SynE (Synthetic data Enhanced Llama-3). We also present the tuning\nexperiments with a relatively small model -- TinyLlama, and employ the derived\nfindings to train the backbone model. Extensive experiments on a number of\nevaluation benchmarks show that our approach can largely improve the\nperformance of the backbone models, including both the general abilities (+8.81\non C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on\nMATH and +4.13 on SciEval), without hurting the original capacities. Our model,\ndata, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE.",
    "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2407.18743.png",
    "numComments": 0,
    "upvoted": false,
    "isAuthorParticipating": false,
    "highlightedTitle": [
      {
        "type": "text",
        "text": "Towards Effective and Efficient Continual Pre-training of Large Language\n  Models"
      }
    ],
    "highlightedSummary": [
      {
        "type": "text",
        "text": "Continual pre-training (CPT) has been an important approach for adapting\nlanguage models to specific domains or tasks. To make the CPT approach more\ntraceable, this paper presents a technical report for continually pre-training\nLlama-3 (8B), which significantly enhances the Chinese language ability and\nscientific reasoning ability of the backbone model. To enhance the new\nabilities while retaining the original abilities, we design specific data\nmixture and curriculum strategies by utilizing existing datasets and\nsynthesizing high-quality datasets. Specifically, we synthesize\nmultidisciplinary scientific question and answer (QA) pairs based on related\nweb pages, and subsequently incorporate these synthetic data to improve the\nscientific reasoning ability of Llama-3. "
      },
      {
        "type": "highlight",
        "text": "We refer to the model after CPT as\nLlama-3-SynE (Synthetic data Enhanced Llama-3)."
      },
      {
        "type": "text",
        "text": " We also present the tuning\nexperiments with a relatively small model -- TinyLlama, and employ the derived\nfindings to train the backbone model. Extensive experiments on a number of\nevaluation benchmarks show that our approach can largely improve the\nperformance of the backbone models, including both the general abilities (+8.81\non C-Eval and +6.31 on CMMLU) and the scientific reasoning abilities (+12.00 on\nMATH and +4.13 on SciEval), without hurting the original capacities. Our model,\ndata, and codes are available at https://github.com/RUC-GSAI/Llama-3-SynE."
      }
    ]
  }
]
